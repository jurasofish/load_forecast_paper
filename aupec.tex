\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Day Ahead Load Forecasting for the Modern Distribution Network - A Case Study in the Tasmanian Distribution Network}

\author{\IEEEauthorblockN{Michael Jurasovic}
\IEEEauthorblockA{\textit{Engineering~~} \\
\textit{UTAS~~}\\
Hobart~ \\
mjj4@utas.edu.au}
\and
\IEEEauthorblockN{Evan Franklin}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address}
\and
\IEEEauthorblockN{Michael Negnevitsky}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address}
\and
\IEEEauthorblockN{Paul Scott}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address}
}

\maketitle

\begin{abstract}
The transformer neural network architecture was applied to short term load forecasting.
\end{abstract}

\begin{IEEEkeywords}
component, formatting, style, styling, insert
\end{IEEEkeywords}

\section{Introduction}
The modern distribution network has changed more over the last ten years than it has in the previous hundred.
In the past, generation and load were largely separate; power was generated exclusively at large stations, and power was consumed by customers after traversing the transmission and distribution networks. 
These days, power is still consumed in the distribution network, but is also generated and manipulated by distributed energy resources (DER). 
\par
DERs are controllable devices in the power network that generate, store, and consume load. 
This includes solar generation (PV), battery storage, and electric vehicles (EV). 
\par
The Tasmanian distribution network is forecast to experience significant increases in these technologies by 2025: \\
\begin{itemize}
	\item 600\% increase in battery storage capacity (from 100MWh to 600MWh) \cite{Jacobs2017}
	\item 170\% increase in PV installation capacity (from 130MW to 220MW) \cite{Jacobs2017}
	\item 39\% of new car sales with be EVs - the highest in the country \cite{AEMO2016}
\end{itemize}

This changing network presents an opportunity to maximize the use of existing assets by delaying the need for network augmentations, while also providing customers with a more reliable supply of power.
For example, batteries could be used to peak-shift, reducing maximum feeder load.
However, to achieve this requires sophisticated methods to optimize the power flow to and from the distributed resources.
\par
One method to achieve this is presented in \cite{Scott2014} and has been implemented on Bruny Island, Tasmania.
The island is a popular holiday destination and during peak periods, such as Easter morning/afternoon peaks, the submarine feeder supplying the island becomes overloaded and has to be supplemented by a diesel generator located on the island.
The aim of the project was to peak shift the load away from the morning/afternoon and avoid the use of the generator.

The method relies on having an accurate forecast of day-ahead load at the feeder level.
Load forecasting methods commonly employed in industry are neither intended to forecast with high accuracy over a time period this short nor at such a low level in the distribution network \cite{CIGRE2016}.
As a result, the optimization of the distributed resources was not as effective as it could be.
\par
To solve this, a neural network-based load forecasting system is proposed.
This system will be applied to Bruny Island, in southern Tasmania, as a case study.
Bruny Island currently has a high penetration of PV and battery technology which is optimized by the Network-aware Coordination (NAC) algorithm \cite{Evan2016}.
Additionally, it is constrained by its feeder during peak holiday periods, necessitating an on-island diesel generator. The load forecasting system will need to be able to perform equally well on holidays, where the load is generally large, and on normal days. 
This makes it a perfect case study to highlight the potential benefits that distributed resources can have in the network.
This project and case study is supported by TasNetworks.


\section{Proposed Forecasting System}
Recurrent Neural Networks (RNN) have recently been popular for load forecasting \cite{Kong2018}.
However, RNNs have been out-performed by the Transformer \cite{Vaswani2017} model in several domains including machine translation \cite{Vaswani2017} (where the architecture was first proposed and applied), medical time series forecasting and regression \cite{Song2017}, and image generation \cite{Parmar2018}.
\par
The proposed forecasting system uses a Transformer neural network model combined with similar profile selection.
\par
Given a multivariate time series $X = (x_1, ..., x_T)$, with $x_t \in \mathbb{R}^N$ denoting a point in time comprised of N observations (a single point in an N-variate time series), the forecasting system produces a univariate time series forecast $Y = (y_1, ..., y_U)$ with $y_t \in \mathbb{R}^1$.
\par
The points in $X$ and $Y$ are evenly spaced, e.g. 30 or 60 minutes apart, and this is the same for both $X$ and $Y$.
$X$ and $Y$ are aligned on any five minute interval, allowing the forecast to be updated every five minutes.


\subsection{Similar Profile Selection}
Load profiles are influenced by exogenous factors such as weather, day of the week, and holiday status \cite{Weron2006}.
Holiday status indicates which holiday the period is in - Easter or Christmas for example.
These different holidays are assigned different integer identifiers (with no holiday assigned identifier 0) to form a time series.
The forecasting system was provided with historical load profiles with similar exogenous data to the profile being forecast.
\par
Similar profiles were identified by first finding candidate similar profiles an integer multiple of 1 year $\pm$30 days away from the profile being forecast and filtering the candidates down to profiles with exactly matching hour and minute.
The hour and minute used were in local time to account for changes around daylight savings.
Then the weighted euclidean distance between the profile being forecast and each candidate profile was calculated using the following features: max temp, min temp, day of week, holiday type, and previous day maximum load. \textbf{(reconcile with code).}
The candidate profiles with the lowest distance were selected to be used as input, and their corresponding historical weather data was also supplied as an input.
\par
when training and testing the similar days were selected from both the past and the future, as the training data for the system is only five years.
\subsection{Transformer}
The transformer neural network architecture, shown in figure \ref{fig:transformer}, was introduced by \cite{Vaswani2017} in 2017 and at the time was the state of the art in neural machine translation.
This architecture follows the standard sequence-to-sequence/encoder-decoder architecture: the encoder transforms an input $X = (x_1, ..., x_T)$ into a latent representation $Z = (z_1, ..., z_v)$, and the decoder transforms $Z$ into an output sequence $Y = (y_1, ..., y_U)$.

The encoder is constructed of a stack of $L$ identical layers, each containing two sub-layers.
The first is multi-head self-attention and the second is a feedfoward network, both discussed below.
Both sub-layers have a residual connection around them and are fed into a normalization layer.

The decoder is similar to the encoder except for a third layer which implements multi-head attention on the outputs of the encoder.
The input to the decoder is the output of the decoder, but shifted right by one.
This requires an iterative approach to be used to predict all points in the time series.
The self-attention in the decoder is masked so that when producing an output at time $t$ it can only see data from prior to $t$.

It was adapted by \cite{Song2017} to perform time series forecasting in a medical context. \textbf{(Maybe ignore this?)}
The individual sections of the transformer are discussed in the following sections.

\begin{figure}[htbp]
	\centerline{\includegraphics[width=.35\textwidth]{images/transformer_vaswani_copied.png}}
	\caption{Transformer architecture. This has been copied from \cite{Vaswani2017} - todo reproduce.}
	\label{fig:transformer}
\end{figure}

\subsection{Input Embedding}
The input $X$ is a $T \times N$ matrix, where the rows represent $T$ points in time, and the columns represent $N$ time series.
This is embedded by applying a one-dimensional convolution to each row to produce an embedded $X'$ of dimension $T \times d$.
This is intended to allow the neural network to learn the relationships and dependencies between the different input time series.
Given a kernel $K$ of dimension $T \times d$, the embedded representation is given by $X'_{i, j} = X_{i,*} \cdot K_{*, j}$.

\subsection{Positional Encoding}
The model has no way of telling the position or order of each element in the input, so this information is injected in the positional encoding step.
This is done by using a trainable lookup table to add the same value to the inputs at both test and train time depending on their position in time in the input.
Specifically, a matrix lookup table of embeddings $E$ of dimension $T \times d$ is added to the embedded inputs such that $X'_{i,*} = X'_{i,*} + E_{i,*}$.

\subsection{Multi-Head Attention}
Scaled dot-product attention used multiple times, concatenated, and put through feedforward.
Present a few equations, then explain intuitively.

\subsection{Feed-forward}
The feed-forward network is a two layer network that is applied identically to each time step.
Given an input $X$ of dimension $T \times d$, the output Y is populated by $Y_{i,*} = $ max$(0, X_{i,*} \cdot W_1 + b_1) \cdot W_2 + b_2$ with $Y$ having the same dimensions as $X$.
$W_1$ is a matrix of dimension $d \times 4d$, $b_1$ is a vector of dimension $4d$, $W_2$ is a matrix of dimension $4d \times d$, $b_2$ is a vector of dimension $d$.

\subsection{Addition \& Normalization}
Residual connections \cite{He2015} are applied around each sub-layer.
That is, the output of each sub-layer is $x + $subLayer$(x)$ where subLayer$(x)$ is the original output of the sub-layer.
The outputs are then normalized by applying layer normalization \cite{Ba2016}.

\subsection{Dropout and regularization}
Dropout is applied in the following positions:
\begin{itemize}
	\item At the output of the positional encoding. 
	\item At the output of each multi-head attention sub-layer.
\end{itemize}


\section{Case Study}
The forecasting system was applied to Bruny Island.
Also discuss application to a common dataset for comparison with other papers?

The forecasting system was configured with the parameters in table ... .
[Horizon: 24 hours, period: 30 minutes, 5 similar days, ]

\subsection{Data}
Discussion of available data and what was supplied to the forecasting system.

\subsection{Results}
Results of case study.

\section{Conclusion}
Blah Blah the forecaster works.
Maybe something about future work?


\section*{Acknowledgment}
TNW


\bibliographystyle{IEEEtran}
\bibliography{aupec}

\end{document}
